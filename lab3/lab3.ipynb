{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ZSL-KG and Nayak Experiment Set uup\n",
        "\n",
        "1. pip install gdown\n",
        "2. clone zsl and nayak libs\n",
        "3. install requirements \n"
      ],
      "metadata": {
        "id": "TO22iCIgygs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKPqMc8AydK_",
        "outputId": "1e0a805a-43cd-4237-97ec-1556c5da3c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nayak-tmlr22-code'...\n",
            "remote: Enumerating objects: 189, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 189 (delta 52), reused 173 (delta 44), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (189/189), 2.16 MiB | 7.41 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ],
      "source": [
        "# clone nayak first, since it's going to be our main experiment base\n",
        "! pip install gdown\n",
        "! git clone https://github.com/BatsResearch/nayak-tmlr22-code.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgY53pZEuh9R"
      },
      "outputs": [],
      "source": [
        "# take care of setting up nayak experiment \n",
        "%cd nayak-tmlr22-code/\n",
        "# delete the last line of requirement file cus it dont work we will do it manually\n",
        "! sed -i '$d' zsl-kg-requirements.txt\n",
        "! pip install --upgrade pip \n",
        "# somehow numpy 1.22 doesnt exist for google colab so we run with 1.21 \n",
        "! sed -i 's/numpy==1.22.0/numpy==1.21.6/g' zsl-kg-requirements.txt\n",
        "! pip install -r zsl-kg-requirements.txt\n",
        "! git clone https://github.com/BatsResearch/zsl-kg.git\n",
        "%cd zsl-kg/\n",
        "! pip install . \n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intent Classification \n",
        "\n",
        "1. cd into intent classification \n",
        "2. create folders for data\n",
        "3. download snips subgraph\n",
        "4. download glove dataset \n",
        "5. run main experiments "
      ],
      "metadata": {
        "id": "DxQA_G-9y8pu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mU_B-8PyuVa",
        "outputId": "9ce5aeaf-eb1e-421d-ef2a-e053ffd068ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'intent_classification/'\n",
            "/content/nayak-tmlr22-code/intent_classification\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/subgraphs’: File exists\n"
          ]
        }
      ],
      "source": [
        "# set up intent classification \n",
        "%cd intent_classification/\n",
        "! mkdir data\n",
        "! mkdir data/subgraphs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download snips subgraph for intent classification \n",
        "# snips subgraph https://drive.google.com/file/d/1id767oIDz0pZyd7OG5GbGTbOHlJia6cG/view?usp=sharing\n",
        "! gdown https://drive.google.com/uc?id=1id767oIDz0pZyd7OG5GbGTbOHlJia6cG\n",
        "! mv snips_graph.zip data/subgraphs/snips_graph.zip \n",
        "! unzip data/subgraphs/snips_graph.zip -d data/subgraphs/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSoDZZkEuX-5",
        "outputId": "34e2d0ce-07d2-412a-d57e-32c2f6d2e703"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gdown/cli.py\", line 166, in main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gdown/download.py\", line 146, in download\n",
            "    res = sess.get(url, headers=headers, stream=True, verify=verify)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 543, in get\n",
            "    return self.request('GET', url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/requests/adapters.py\", line 449, in send\n",
            "    timeout=timeout\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 677, in urlopen\n",
            "    chunked=chunked,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 426, in _make_request\n",
            "    six.raise_from(e, None)\n",
            "  File \"<string>\", line 3, in raise_from\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\", line 421, in _make_request\n",
            "    httplib_response = conn.getresponse()\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1373, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 319, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 280, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.7/ssl.py\", line 1071, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.7/ssl.py\", line 929, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "mv: cannot stat 'snips_graph.zip': No such file or directory\n",
            "Archive:  data/subgraphs/snips_graph.zip\n",
            "replace data/subgraphs/snips_graph/train_graph/params.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download snips train test validation dataset\n",
        "# dev https://drive.google.com/file/d/1g02oOWf_L4MAxP-NpDKG-cUgfReU9Yfu/view?usp=sharing\n",
        "# test https://drive.google.com/file/d/1QTlLQBynNShu_mNsTikRXsVV4CH73dRr/view?usp=sharing\n",
        "# train https://drive.google.com/file/d/15OW34XpKISmrarzybuirFbqA51qj_2Gm/view?usp=sharing\n",
        "! mkdir data/snips\n",
        "! gdown https://drive.google.com/uc?id=1g02oOWf_L4MAxP-NpDKG-cUgfReU9Yfu\n",
        "! mv dev.txt data/snips/dev.txt\n",
        "! gdown https://drive.google.com/uc?id=1QTlLQBynNShu_mNsTikRXsVV4CH73dRr\n",
        "! mv test.txt data/snips/test.txt\n",
        "! gdown https://drive.google.com/uc?id=15OW34XpKISmrarzybuirFbqA51qj_2Gm\n",
        "! mv train.txt data/snips/train.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Rw2VtJzpjF",
        "outputId": "2f4aaad7-3fec-45de-edaa-4e6250d16443"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data/snips’: File exists\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g02oOWf_L4MAxP-NpDKG-cUgfReU9Yfu\n",
            "To: /content/nayak-tmlr22-code/intent_classification/dev.txt\n",
            "100% 203k/203k [00:00<00:00, 85.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QTlLQBynNShu_mNsTikRXsVV4CH73dRr\n",
            "To: /content/nayak-tmlr22-code/intent_classification/test.txt\n",
            "100% 204k/204k [00:00<00:00, 73.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15OW34XpKISmrarzybuirFbqA51qj_2Gm\n",
            "To: /content/nayak-tmlr22-code/intent_classification/train.txt\n",
            "100% 343k/343k [00:00<00:00, 101MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op_5U9E5Vo_x",
        "outputId": "47b57f55-8880-44ba-833d-fd336a75f40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-16 13:53:00--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2022-10-16 13:53:01--  https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip  19%[==>                 ] 403.96M  5.07MB/s    eta 5m 24s ^C\n",
            "Archive:  data/glove.840B.300d.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of data/glove.840B.300d.zip or\n",
            "        data/glove.840B.300d.zip.zip, and cannot find data/glove.840B.300d.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "# download Glove Data for training \n",
        "! wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "! mv glove.840B.300d.zip data/\n",
        "! unzip -j data/glove.840B.300d.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_dPq0XYfFQo",
        "outputId": "0c463c1d-2b6d-4b07-f7a2-77b65f080e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************\n",
            "Training Details\n",
            "DATASET:  snips\n",
            "ENCODER:  trgcn\n",
            "********************\n",
            "5974it [00:00, 24368.27it/s]\n",
            "3914it [00:00, 18879.66it/s]\n",
            "3914it [00:00, 17192.81it/s]\n",
            "100% 13802/13802 [00:00<00:00, 141248.68it/s]\n",
            "Loading GloVe...\n",
            "word embeddings created...\n",
            "Loading the text encoder...\n",
            "Loading the label encoder...\n",
            "**** Epoch 0 ****\n",
            "  1% 1/187 [00:01<05:48,  1.87s/it]\n",
            "\n",
            "Aborted!\n"
          ]
        }
      ],
      "source": [
        "# --dataset does not work lol there's no option in it\n",
        "! python train.py --label_encoder_type trgcn --seed 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Set up file location to go into to run experiments \n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "M_DIR_PATH = os.path.dirname(os.path.realpath(os.getcwd()))\n",
        "INTENT_PATH = M_DIR_PATH + '/intent_classification'\n",
        "OBJECT_PATH = M_DIR_PATH + '/object_classification'\n",
        "FGET_PATH = M_DIR_PATH + '/fget'\n",
        "os.chdir(INTENT_PATH)\n"
      ],
      "metadata": {
        "id": "8Yepzlka4zV0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "oFAWHYOdg9UI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "we want to run the same experiment several times while \n",
        "keeping the result, so i added the intent classification folder\n",
        "to set up experiment \n",
        "I don't really liked nayak way of experimentation since it's just\n",
        "1 experiment without creating results for me in file.\n",
        "so i become very hacky and basically set working directory to be\n",
        "in intent classification and then do main and train_epoch function\n",
        "so that they return me results and run the experiment multiple times\n",
        "\n",
        "yes this is hacky but I have 1 week left to do this \n",
        "\"\"\"\n",
        "from train import *\n",
        "def train_epochs_2(model, iterator, optimizer, datasets, options, epochs=10):\n",
        "    \"\"\"The function is used to train the model for a given number of epochs\"\"\"\n",
        "    val_loss = []\n",
        "\n",
        "    train_dataset, dev_dataset, test_dataset = tuple(datasets)\n",
        "    if options[\"label_encoder_type\"] in [\n",
        "        \"gcn\",\n",
        "        \"gat\",\n",
        "        \"rgcn\",\n",
        "        \"lstm\",\n",
        "        \"trgcn\",\n",
        "    ]:\n",
        "        train_graph, dev_graph, test_graph = get_graph(options[\"graph_path\"])\n",
        "        # move to device\n",
        "        train_graph.to(options[\"device\"])\n",
        "        dev_graph.to(options[\"device\"])\n",
        "        test_graph.to(options[\"device\"])\n",
        "        # get idx\n",
        "        train_idx = train_graph.get_node_ids(concept_maps[\"train\"])\n",
        "        dev_idx = train_graph.get_node_ids(concept_maps[\"dev\"])\n",
        "        test_idx = train_graph.get_node_ids(concept_maps[\"test\"])\n",
        "    else:\n",
        "        train_graph = None\n",
        "        dev_graph = None\n",
        "        test_graph = None\n",
        "        wn_mapping = pd.read_csv(\n",
        "            os.path.join(DIR_PATH, \"misc_data/snips_mapping.csv\")\n",
        "        )\n",
        "        graph = json.load(\n",
        "            open(os.path.join(DIR_PATH, \"data/induced_graph.json\"), \"r\")\n",
        "        )\n",
        "        wnids = graph[\"wnids\"]\n",
        "        wnid_to_idx = dict([(wnid, idx) for idx, wnid in enumerate(wnids)])\n",
        "        label_idx = [wnid_to_idx[wn_mapping[\"wnid\"][i]] for i in range(7)]\n",
        "        train_idx = label_idx[:3]\n",
        "        dev_idx = label_idx[3:5]\n",
        "        test_idx = label_idx[5:]\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"**** Epoch {epoch} ****\")\n",
        "        model, optimizer = train_model(\n",
        "            model,\n",
        "            train_dataset,\n",
        "            iterator,\n",
        "            optimizer,\n",
        "            train_graph,\n",
        "            torch.tensor(train_idx).to(options[\"device\"]),\n",
        "        )\n",
        "\n",
        "        loss = compute_loss(\n",
        "            model,\n",
        "            dev_dataset,\n",
        "            iterator,\n",
        "            dev_graph,\n",
        "            torch.tensor(dev_idx).to(options[\"device\"]),\n",
        "        )\n",
        "        print(f\"Val Loss {loss}\")\n",
        "\n",
        "        if epoch > 0:\n",
        "            if loss < min(val_loss):\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        else:\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        val_loss.append(loss)\n",
        "\n",
        "        test_model(\n",
        "            model,\n",
        "            test_dataset,\n",
        "            iterator,\n",
        "            test_graph,\n",
        "            torch.tensor(test_idx).to(options[\"device\"]),\n",
        "        )\n",
        "\n",
        "    save_path = get_save_path(model.options[\"model_path\"], options)\n",
        "    torch.save(best_model_wts, save_path)\n",
        "\n",
        "    print(\"done with model training\")\n",
        "    print(\"loading best model\")\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    result = test_model(\n",
        "        model,\n",
        "        test_dataset,\n",
        "        iterator,\n",
        "        test_graph,\n",
        "        torch.tensor(test_idx).to(options[\"device\"]),\n",
        "    )\n",
        "    print(\"done!\")\n",
        "    return (\n",
        "      model,\n",
        "      test_dataset,\n",
        "      iterator,\n",
        "      test_graph,\n",
        "      torch.tensor(test_idx).to(options[\"device\"]),\n",
        "      result \n",
        "    )\n",
        "def main_loop(label_encoder_type, \n",
        "              seed=0, \n",
        "              lr=0.001, \n",
        "              decay=1e-05, \n",
        "              gpu=0, \n",
        "              count=30):\n",
        "    \"\"\"\n",
        "    The function is used to setup and train the model for the dataset\n",
        "    and the encoder type; the function trains a bilinear model\n",
        "    with a bilstm text encoder with the label encoder mentioned in the\n",
        "    parameter\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(count):\n",
        "      print(\"*\" * 20)\n",
        "      print(\"Training Details\")\n",
        "      print(\"DATASET: \", \"snips\")\n",
        "      print(\"ENCODER: \", label_encoder_type)\n",
        "      print(\"*\" * 20)\n",
        "\n",
        "      model_path = os.path.join(DIR_PATH, \"data/models/snips\")\n",
        "\n",
        "      # create directory for saving the model\n",
        "      if not os.path.exists(model_path):\n",
        "          os.makedirs(model_path)\n",
        "\n",
        "      set_seed(seed)\n",
        "      device, cuda_device = init_device(gpu)\n",
        "\n",
        "      options = {\n",
        "          \"label_encoder_type\": label_encoder_type,\n",
        "          \"lr\": lr,\n",
        "          \"dataset\": \"snips\",\n",
        "          \"joint_dim\": JOINT_DIM,\n",
        "          \"decay\": decay,\n",
        "          \"seed\": seed,\n",
        "          \"graph_path\": os.path.join(DIR_PATH, \"data/subgraphs/snips_graph\"),\n",
        "          \"model_path\": model_path,\n",
        "          \"gpu\": gpu,\n",
        "          \"device\": device,\n",
        "          \"cuda_device\": cuda_device,\n",
        "      }\n",
        "\n",
        "      # get the vocab and everything else for training the models\n",
        "      model, iterator, optimizer, datasets = setup(label_encoder_type, options)\n",
        "\n",
        "      model, dataset, iterator, kg, label_idx, result = train_epochs(model, iterator, optimizer, datasets, options, epochs=10)\n",
        "      results.append(result['unseen_acc'])\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trgcn_result = main_loop(label_encoder_type='trgcn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3WUd80o9KY-",
        "outputId": "8222e8bd-1afd-4252-fd97-886d9adedfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************\n",
            "Training Details\n",
            "DATASET:  snips\n",
            "ENCODER:  trgcn\n",
            "********************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5974it [00:00, 14166.18it/s]\n",
            "3914it [00:00, 10467.70it/s]\n",
            "3914it [00:00, 39883.55it/s]\n",
            "100%|██████████| 13802/13802 [00:00<00:00, 90945.88it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe...\n",
            "word embeddings created...\n",
            "Loading the text encoder...\n",
            "Loading the label encoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/187 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** Epoch 0 ****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [02:24<00:00,  1.29it/s]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss = 39.9453350257827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 123/123 [00:25<00:00,  4.80it/s]\n",
            "  2%|▏         | 2/123 [00:00<00:09, 12.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss 94.85520935058594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 123/123 [00:10<00:00, 11.27it/s]\n",
            "  0%|          | 0/187 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unseen acc =  0.8988\n",
            "**** Epoch 1 ****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187/187 [02:20<00:00,  1.33it/s]\n",
            "  1%|          | 1/123 [00:00<00:22,  5.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss = 2.3628473032149486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▊    | 72/123 [00:13<00:09,  5.23it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rpdVFDCxFspC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TO22iCIgygs7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}